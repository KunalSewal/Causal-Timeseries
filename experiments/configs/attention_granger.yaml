# Attention Granger Configuration

# Data settings
data:
  path: 'data/processed/stock_prices.csv'
  lag: 10  # Longer lag for attention to be useful
  horizon: 1
  split_ratios: [0.7, 0.15, 0.15]

# Model settings
model:
  name: 'AttentionGranger'
  num_vars: 4
  hidden_dim: 128
  num_layers: 3
  dropout: 0.3
  device: 'cuda'

# Training settings
training:
  epochs: 150
  batch_size: 64
  learning_rate: 0.0005
  scheduler:
    type: 'ReduceLROnPlateau'
    patience: 10
    factor: 0.5
  early_stopping:
    patience: 20
    min_delta: 0.0001

# Evaluation settings
evaluation:
  metrics: ['mse', 'mae', 'r2']
  threshold: 0.35
  visualize_attention: true  # Save attention weight plots

# Output settings
output:
  model_dir: 'experiments/results/model_outputs'
  graph_dir: 'experiments/results/graphs'
  attention_dir: 'experiments/results/attention'
  log_dir: 'experiments/results/logs'
  save_frequency: 10

# Random seed
seed: 42
